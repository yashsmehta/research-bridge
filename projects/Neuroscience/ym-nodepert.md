<!-- Project Title-->
#### On the Limitations of Perturbation Based Methods for Training Deep Networks
<!-- Please do not change the font size-->
<font size="2"> 

<!-- Your Name and Affiliation-->
(30th June '20) Yash Mehta, Gatsby Computational Neuroscience Unit, UCL

**Abstract** (2 Sentence Summary)
<p>

What algorithms drive goal directed learning in the brain? We aim to understand how perturbation methods scale as the size and architecture of the network changes. Our empirical results with perturbation methods on modern architectures and datasets suggest that they scale even more poorly than is suggested by theoretical results.

</p>

<!-- Project Information Table-->
| Field | Information          |
| ------------- | ----------- |
| **Interested in**        |  Currently not seeking a collaborator, but open to discuss our progress and the results with interested researchers  | <!-- e.g. Actively looking for collaborators, Not looking but open to collaborating, Discussing research and project results
| **Other project members**  | Naoki Hiratani (Gatsby), Peter Latham (Gatsby), Timothy Lillicrap (DeepMind)|
| **Current project stage**         |  Near Completion  | <!-- for e.g. Initial brainstorming and ideation phase, Just started, Middle, Almost Complete, Complete
| **Contact**     |  yashsmehta95@gmail.com  | <!-- Twitter, Email, etc.